---
title: "Constructing Non-Hierarchical P/NBD Models"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "Last updated: `r format(Sys.time(), '%B %d, %Y')`"
editor: source
execute:
  message: false
  warning: false
  error: false
format:
  html:
    light: superhero
    dark: darkly
    anchor-sections: true
    embed-resources: true
    number-sections: true
    smooth-scroll: true
    toc: true
    toc-depth: 3
    toc-location: left
    code-fold: true
    code-summary: "Show code"
---


```{r import_libraries}
#| echo: FALSE
#| message: FALSE

library(conflicted)
library(tidyverse)
library(scales)
library(cowplot)
library(directlabels)
library(magrittr)
library(rlang)
library(fs)
library(purrr)
library(furrr)
library(glue)
library(cmdstanr)
library(brms)
library(posterior)
library(bayesplot)
library(tidybayes)


source("lib_utils.R")
source("lib_btyd.R")


conflict_lst <- resolve_conflicts(
  c("magrittr", "rlang", "dplyr", "readr", "purrr", "ggplot2", "MASS",
    "fitdistrplus")
  )

options(
  width = 80L,
  warn  = 1,
  mc.cores = parallel::detectCores()
  )


set.seed(42)
stanfit_seed <- 4100

open_syslog("construct_shortsynth_fixed_pnbd_models")

theme_set(theme_cowplot())
plan(multisession)
```

In this workbook we expand on our initial exploration of the P/NBD models and
build some infrastructure allowing us to build and fit multiple models with
different sets of data.



# Load and Construct Datasets

We start by modelling the P/NBD model using our synthetic datasets before we
try to model real-life data.

```{r set_start_end_dates}
use_fit_start_date <- as.Date("2020-01-01")
use_fit_end_date   <- as.Date("2022-01-01")

use_valid_start_date <- as.Date("2022-01-01")
use_valid_end_date   <- as.Date("2023-01-01")
```



## Load Short Time-frame Synthetic Data

We now want to load the short time-frame synthetic data.


```{r load_shortframe_synthetic_data}
#| echo: TRUE

customer_cohortdata_tbl <- read_rds("data/synthdata_shortframe_cohort_tbl.rds")
customer_cohortdata_tbl |> glimpse()

customer_simparams_tbl  <- read_rds("data/synthdata_shortframe_simparams_tbl.rds")
customer_simparams_tbl |> glimpse()

customer_transactions_tbl <- read_rds("data/synthdata_shortframe_transactions_tbl.rds")
customer_transactions_tbl |> glimpse()
```


We re-produce the visualisation of the transaction times we used in previous
workbooks.

```{r plot_customer_transaction_times}
#| echo: TRUE

plot_tbl <- customer_transactions_tbl |>
  group_nest(customer_id, .key = "cust_data") |>
  filter(map_int(cust_data, nrow) > 3) |>
  slice_sample(n = 30) |>
  unnest(cust_data)

ggplot(plot_tbl, aes(x = tnx_timestamp, y = customer_id)) +
  geom_line() +
  geom_point() +
  labs(
      x = "Date",
      y = "Customer ID",
      title = "Visualisation of Customer Transaction Times"
    ) +
  theme(axis.text.y = element_text(size = 10))
```



## Construct Datasets

Having loaded the synthetic data we need to construct a number of datasets of
derived values.

```{r construct_summary_stats_data}
#| echo: TRUE

customer_summarystats_tbl <- customer_transactions_tbl |>
  calculate_transaction_cbs_data(last_date = use_fit_end_date)

customer_summarystats_tbl |> glimpse()
```

As before, we construct a number of subsets of the data for use later on with
the modelling and create some data subsets.


```{r construct_data_subset_id}
#| echo: TRUE

shuffle_tbl <- customer_summarystats_tbl |>
  slice_sample(prop = 1, replace = FALSE)

id_50    <- shuffle_tbl |> head(50)    |> pull(customer_id) |> sort() 
id_1000  <- shuffle_tbl |> head(1000)  |> pull(customer_id) |> sort()
id_5000  <- shuffle_tbl |> head(5000)  |> pull(customer_id) |> sort()
id_10000 <- shuffle_tbl |> head(10000) |> pull(customer_id) |> sort()
```

We then construct some fit data based on these values.

```{r construct_fit_subset_data}
#| echo: TRUE

fit_1000_data_tbl  <- customer_summarystats_tbl |> filter(customer_id %in% id_1000)
fit_1000_data_tbl |> glimpse()

fit_10000_data_tbl <- customer_summarystats_tbl |> filter(customer_id %in% id_10000)
fit_10000_data_tbl |> glimpse()
```


Finally, we also want to recreate our transaction visualisation for the first
50 customers randomly selected.

```{r plot_customer_transaction_times_first50}
#| echo: TRUE

plot_tbl <- customer_transactions_tbl |>
  filter(customer_id %in% id_50)

ggplot(plot_tbl, aes(x = tnx_timestamp, y = customer_id)) +
  geom_line() +
  geom_point() +
  labs(
      x = "Date",
      y = "Customer ID",
      title = "Visualisation of Customer Transaction Times"
    ) +
  theme(axis.text.y = element_text(size = 10))
```


## Write Data

```{r write_data_disk}
#| echo: TRUE

id_1000  |> write_rds("data/shortframe_id_1000.rds")
id_5000  |> write_rds("data/shortframe_id_5000.rds")
id_10000 |> write_rds("data/shortframe_id_10000.rds")

fit_1000_data_tbl  |> write_rds("data/fit_1000_shortframe_data_tbl.rds")
fit_10000_data_tbl |> write_rds("data/fit_10000_shortframe_data_tbl.rds")

customer_summarystats_tbl |> write_rds("data/customer_summarystats_shortframe_tbl.rds")
```



# Fit First Fixed-Prior P/NBD Model

We now construct our Stan model and prepare to fit it with our synthetic
dataset.

Before we start on that, we set a few parameters for the workbook to organise
our Stan code.

```{r setup_workbook_parameters}
#| echo: TRUE

stan_modeldir <- "stan_models"
stan_codedir  <-   "stan_code"
```


We start with the Stan model.

```{r display_pnbd_fixed_model_stancode}
#| echo: FALSE

read_lines("stan_code/pnbd_fixed.stan") |> cat(sep = "\n")
```

This file contains a few new features of Stan - named file includes and
user-defined functions - `calculate_pnbd_loglik`. We look at this file here:

```{r display_util_functions_stancode}
#| echo: FALSE

read_lines("stan_code/util_functions.stan") |> cat(sep = "\n")
```

We now compile this model using `CmdStanR`.

```{r compile_pnbd_fixed_stanmodel}
#| echo: TRUE
#| results: "hide"

pnbd_fixed_stanmodel <- cmdstan_model(
  "stan_code/pnbd_fixed.stan",
  include_paths =   stan_codedir,
  pedantic      =           TRUE,
  dir           =  stan_modeldir
  )
```


We then use this compiled model with our data to produce a fit of the data.

```{r fit_pnbd_fixed_stanmodel}
#| echo: TRUE
#| cache: TRUE

stan_modelname <- "pnbd_nonhier_fixed"
stanfit_prefix <- str_c("fit_", stan_modelname) 
stanfit_seed   <- stanfit_seed + 1

stan_data_lst <- customer_fit_stats_tbl |>
  select(customer_id, x, t_x, T_cal) |>
  compose_data(
    lambda_mn = 0.25,
    lambda_cv = 1.00,
    
    mu_mn     = 0.10,
    mu_cv     = 1.00,
    )

pnbd_nonhier_fixed_stanfit <- pnbd_fixed_stanmodel$sample(
  data            =                stan_data_lst,
  chains          =                            4,
  iter_warmup     =                          500,
  iter_sampling   =                          500,
  seed            =                 stanfit_seed,
  save_warmup     =                         TRUE,
  output_dir      =                stan_modeldir,
  output_basename =               stanfit_prefix,
  )

pnbd_nonhier_fixed_stanfit$summary()
```


We have some basic HMC-based validity statistics we can check.

```{r calculate_pnbd_fixed_hmc_diagnostics}
#| echo: TRUE

pnbd_nonhier_fixed_stanfit$cmdstan_diagnose()
```



## Visual Diagnostics of the Sample Validity

We need to check some diagnostics to assess the validity of the sample fit from
the MCMC work, but our visualisations will not be as comprehensive for this
workbook, just showing a summary of the plots. In particular we show the
traceplots and $N_{eff}$.

```{r plot_pnbd_nonhier_fixed_diagnostics}
#| echo: TRUE

parameter_subset <- c(
  "lambda[1]", "lambda[2]", "lambda[3]", "lambda[4]",
  "mu[1]",     "mu[2]",     "mu[3]",     "mu[4]"
  )

pnbd_nonhier_fixed_stanfit$draws(inc_warmup = FALSE) |>
  mcmc_trace(pars = parameter_subset) +
  expand_limits(y = 0) +
  labs(
    x = "Iteration",
    y = "Value",
    title = "Traceplot of Sample of Lambda and Mu Values"
    ) +
  theme(axis.text.x = element_text(size = 10))


pnbd_nonhier_fixed_stanfit |>
  neff_ratio(pars = c("lambda", "mu")) |>
  as.numeric() |>
  mcmc_neff() +
    ggtitle("Plot of Parameter Effective Sample Sizes")
```



## Check Model Fit

As we are still working with synthetic data, we know the true values for each
customer and so we can check how good our model is at recovering the true
values on a customer-by-customer basis.

As in previous workbooks, we build our validation datasets and then check the
distribution of $q$-values for both $\lambda$ and $\mu$ across the customer
base.


```{r construct_pnbd_fixed_validation_qvalues}
#| echo: TRUE

pnbd_nonhier_fixed_valid_lst <- create_pnbd_posterior_validation_data(
  stanfit       = pnbd_nonhier_fixed_stanfit,
  data_tbl      = customer_fit_stats_tbl,
  simparams_tbl = customer_simparams_tbl
  )

pnbd_nonhier_fixed_valid_lst$lambda_qval_plot |> plot()

pnbd_nonhier_fixed_valid_lst$mu_qval_plot |> plot()
```

These plots looks like the model is recovering the parameters well, but cannot
rely on this approach once we use real data so we will stop using this now.

Instead, we will look at using our posterior sample to generate data and
compare this simulated data against the data we fit. This procedure is similar
to what we did before but now we focus on in sample data rather than using
validation data.


```{r calculate_pnbd_posterior_simstats}
#| echo: TRUE
#| cache: TRUE

sim_stats_tbl <- construct_pnbd_posterior_statistics(
  stanfit         = pnbd_nonhier_fixed_stanfit,
  fitdata_tbl     = customer_fit_stats_tbl
  )

sim_stats_tbl |> glimpse()
```

We then use these posterior statistics as inputs to our simulations to help
us assess the in-sample quality of fit.


```{r generate_pnbd_nonhier_fixed_fitdata_transactions}
#| echo: TRUE

plan(multisession)

fit_label <- "pnbd_nonhier_fixed"

precompute_dir <- glue("precompute/{fit_label}")

ensure_exists_precompute_directory(precompute_dir)


precomputed_tbl <- dir_ls(glue("{precompute_dir}")) |>
  as.character() |>
  enframe(name = NULL, value = "sim_file")

pnbd_nonhier_fixed_validsims_index_tbl <- sim_stats_tbl |>
  mutate(
    start_dttm = first_tnx_date,
    end_dttm   = as.Date("")
    end_dttm   = c(),
    lambda     = post_lambda,
    mu         = post_mu,
    tnx_mu     = 1,
    tnx_cv     = 1
    ) |>
  group_nest(customer_id, .key = "cust_params") |>
  mutate(
    sim_file = glue(
      "{precompute_dir}/sims_fit_{fit_label}_{customer_id}.rds"
      )
    )


runsims_tbl <- pnbd_nonhier_fixed_validsims_index_tbl |>
  anti_join(precomputed_tbl, by = "sim_file")



if(nrow(runsims_tbl) > 0) {
  pnbd_nonhier_fixed_validsims_index_tbl <- runsims_tbl |>
    mutate(
      chunk_data = future_map2(
        sim_file, cust_params,
        run_pnbd_simulations_chunk,

        .options = furrr_options(
          globals  = c(
            "calculate_event_times", "rgamma_mucv", "gamma_mucv2shaperate",
            "generate_pnbd_validation_transactions"
            ),
          packages   = c("tidyverse", "fs"),
          scheduling = Inf,
          seed       = 421
          ),
        .progress = TRUE
        )
      )
}


pnbd_nonhier_fixed_validsims_index_tbl |> glimpse()
```

We now want to load up the summary statistics for each of our customers for
later analysis.


```{r retrieve_pnbd_nonhier_fixed_simstats}
#| echo: TRUE

retrieve_sim_stats <- ~ .x |>
  read_rds() |>
  select(draw_id, sim_tnx_count, sim_tnx_last)


pnbd_nonhier_fixed_simstats_tbl <- pnbd_nonhier_fixed_validsims_index_tbl |>
  mutate(
    sim_data = future_map(
      sim_file, retrieve_sim_stats,

      .options = furrr_options(
        globals    = FALSE,
        packages   = c("tidyverse", "fs"),
        scheduling = Inf
        ),
      .progress = TRUE
      )
    ) |>
  select(customer_id, sim_data) |>
  unnest(sim_data)

pnbd_nonhier_fixed_simstats_tbl |> glimpse()
```


# Fit Alternate Prior Model.

We want to try an alternate prior model with a smaller co-efficient of variation
to see what impact it has on our procedures.


```{r fit_pnbd_short_fixed2_stanmodel}
#| echo: TRUE
#| cache: TRUE

stan_modelname <- "pnbd_short_fixed2"
stanfit_prefix <- str_c("fit_", stan_modelname) 
stanfit_seed   <- stanfit_seed + 1


stan_data_lst <- customer_fit_stats_tbl |>
  select(customer_id, x, t_x, T_cal) |>
  compose_data(
    lambda_mn = 0.25,
    lambda_cv = 0.50,
    
    mu_mn     = 0.10,
    mu_cv     = 0.50,
    )

pnbd_short_fixed2_stanfit <- pnbd_fixed_stanmodel$sample(
  data            =                stan_data_lst,
  chains          =                            4,
  iter_warmup     =                          500,
  iter_sampling   =                          500,
  seed            =                 stanfit_seed,
  save_warmup     =                         TRUE,
  output_dir      =                stan_modeldir,
  output_basename =               stanfit_prefix,
  )

pnbd_short_fixed2_stanfit$summary()
```


We have some basic HMC-based validity statistics we can check.

```{r calculate_pnbd_short_fixed2_hmc_diagnostics}
#| echo: TRUE

pnbd_short_fixed2_stanfit$cmdstan_diagnose()
```


## Visual Diagnostics of the Sample Validity

Now that we have a sample from the posterior distribution we need to create a
few different visualisations of the diagnostics.

```{r plot_pnbd_short_fixed2_lambda_traceplots}
#| echo: TRUE

parameter_subset <- c(
  "lambda[1]", "lambda[2]", "lambda[3]", "lambda[4]",
  "mu[1]",     "mu[2]",     "mu[3]",     "mu[4]"
  )

pnbd_short_fixed2_stanfit$draws(inc_warmup = FALSE) |>
  mcmc_trace(pars = parameter_subset) +
  expand_limits(y = 0) +
  labs(
    x = "Iteration",
    y = "Value",
    title = "Traceplot of Sample of Lambda and Mu Values"
    ) +
  theme(axis.text.x = element_text(size = 10))
```


We want to check the $N_{eff}$ statistics also.


```{r plot_pnbd_short_fixed2_parameter_neffratio}
#| echo: TRUE

pnbd_short_fixed2_stanfit |>
  neff_ratio(pars = c("lambda", "mu")) |>
  as.numeric() |>
  mcmc_neff() +
    ggtitle("Plot of Parameter Effective Sample Sizes")
```


## Assess the Model

As we intend to run the same logic to assess each of our models, we have
combined all this logic into a single function `run_model_assessment`, to 
run the simulations and combine the datasets.

### Check In-Sample Data Validation

We first check the model against the in-sample data.

```{r run_pnbd_short_fixed2_fit_assessment}
#| echo: TRUE

assess_data_lst <- run_model_assessment(
  model_stanfit    = pnbd_short_fixed2_stanfit,
  insample_tbl     = customer_fit_stats_tbl,
  outsample_tbl    = customer_valid_stats_tbl,
  fit_label        = "pnbd_short_fixed2",
  fit_end_dttm     = use_fit_end_date     |> as.POSIXct(),
  valid_start_dttm = use_valid_start_date |> as.POSIXct(),
  valid_end_dttm   = use_valid_end_date   |> as.POSIXct(),
  sim_seed         = 420
  )

obs_fitdata_tbl <- customer_fit_stats_tbl |>
  rename(tnx_count = x)
  
insample_plots_lst <- create_model_assessment_plots(
  obsdata_tbl = obs_fitdata_tbl,
  simdata_tbl = assess_data_lst$model_fit_simstats_tbl
  )

insample_plots_lst$multi_plot |> print()
insample_plots_lst$total_plot |> print()
insample_plots_lst$quant_plot |> print()
```

This fit looks reasonable and appears to capture most of the aspects of the
data used to fit it. Given that this is a synthetic dataset, this is not
surprising, but at least we appreciate that our model is valid.


### Check Out-of-Sample Data Validation

We now repeat for the out-of-sample data.

```{r run_pnbd_short_fixed2_valid_assessment}
#| echo: TRUE

### We need to add all the zero count customers into the valid data
obs_validdata_tbl <- customer_fit_stats_tbl |>
  anti_join(customer_valid_stats_tbl, by = "customer_id") |>
  transmute(customer_id, tnx_count = 0) |>
  bind_rows(customer_valid_stats_tbl) |>
  arrange(customer_id)


outsample_plots_lst <- create_model_assessment_plots(
  obsdata_tbl = obs_validdata_tbl,
  simdata_tbl = assess_data_lst$model_valid_simstats_tbl
  )

outsample_plots_lst$multi_plot |> print()
outsample_plots_lst$total_plot |> print()
outsample_plots_lst$quant_plot |> print()
```


# Fit Tight-Lifetime Model

We now want to try a model where we use priors with a tighter coefficient of
variation for lifetime but keep the CoV for transaction frequency.


```{r fit_pnbd_short_fixed4_stanmodel}
#| echo: TRUE
#| cache: TRUE

stan_modelname <- "pnbd_short_fixed4"
stanfit_prefix <- str_c("fit_", stan_modelname)
stanfit_seed   <- stanfit_seed + 1

stan_data_lst <- customer_fit_stats_tbl |>
  select(customer_id, x, t_x, T_cal) |>
  compose_data(
    lambda_mn = 0.25,
    lambda_cv = 1.00,
    
    mu_mn     = 0.10,
    mu_cv     = 0.50,
    )

pnbd_short_fixed4_stanfit <- pnbd_fixed_stanmodel$sample(
  data            =                stan_data_lst,
  chains          =                            4,
  iter_warmup     =                          500,
  iter_sampling   =                          500,
  seed            =                 stanfit_seed,
  save_warmup     =                         TRUE,
  output_dir      =                stan_modeldir,
  output_basename =               stanfit_prefix,
  )

pnbd_short_fixed4_stanfit$summary()
```


We have some basic HMC-based validity statistics we can check.

```{r calculate_pnbd_short_fixed4_hmc_diagnostics}
#| echo: TRUE

pnbd_short_fixed4_stanfit$cmdstan_diagnose()
```


## Visual Diagnostics of the Sample Validity

Now that we have a sample from the posterior distribution we need to create a
few different visualisations of the diagnostics.

```{r plot_pnbd_short_fixed4_lambda_traceplots}
#| echo: TRUE

parameter_subset <- c(
  "lambda[1]", "lambda[2]", "lambda[3]", "lambda[4]",
  "mu[1]",     "mu[2]",     "mu[3]",     "mu[4]"
  )

pnbd_short_fixed4_stanfit$draws(inc_warmup = FALSE) |>
  mcmc_trace(pars = parameter_subset) +
  expand_limits(y = 0) +
  labs(
    x = "Iteration",
    y = "Value",
    title = "Traceplot of Sample of Lambda and Mu Values"
    ) +
  theme(axis.text.x = element_text(size = 10))
```


We want to check the $N_{eff}$ statistics also.


```{r plot_pnbd_short_fixed4_parameter_neffratio}
#| echo: TRUE

pnbd_short_fixed4_stanfit |>
  neff_ratio(pars = c("lambda", "mu")) |>
  as.numeric() |>
  mcmc_neff() +
    ggtitle("Plot of Parameter Effective Sample Sizes")
```


## Assess the Model

As we intend to run the same logic to assess each of our models, we have
combined all this logic into a single function `run_model_assessment`, to 
run the simulations and combine the datasets.

### Check In-Sample Data Validation

We first check the model against the in-sample data.

```{r run_pnbd_short_fixed4_fit_assessment}
#| echo: TRUE

assess_data_lst <- run_model_assessment(
  model_stanfit    = pnbd_short_fixed4_stanfit,
  insample_tbl     = customer_fit_stats_tbl,
  outsample_tbl    = customer_valid_stats_tbl,
  fit_label        = "pnbd_short_fixed4",
  fit_end_dttm     = use_fit_end_date     |> as.POSIXct(),
  valid_start_dttm = use_valid_start_date |> as.POSIXct(),
  valid_end_dttm   = use_valid_end_date   |> as.POSIXct(),
  sim_seed         = 420
  )

obs_fitdata_tbl <- customer_fit_stats_tbl |>
  rename(tnx_count = x)
  
insample_plots_lst <- create_model_assessment_plots(
  obsdata_tbl = obs_fitdata_tbl,
  simdata_tbl = assess_data_lst$model_fit_simstats_tbl
  )

insample_plots_lst$multi_plot |> print()
insample_plots_lst$total_plot |> print()
insample_plots_lst$quant_plot |> print()
```

This fit looks reasonable and appears to capture most of the aspects of the
data used to fit it. Given that this is a synthetic dataset, this is not
surprising, but at least we appreciate that our model is valid.


### Check Out-of-Sample Data Validation

We now repeat for the out-of-sample data.

```{r run_pnbd_short_fixed4_valid_assessment}
#| echo: TRUE

### We need to add all the zero count customers into the valid data
obs_validdata_tbl <- customer_fit_stats_tbl |>
  anti_join(customer_valid_stats_tbl, by = "customer_id") |>
  transmute(customer_id, tnx_count = 0) |>
  bind_rows(customer_valid_stats_tbl) |>
  arrange(customer_id)


outsample_plots_lst <- create_model_assessment_plots(
  obsdata_tbl = obs_validdata_tbl,
  simdata_tbl = assess_data_lst$model_valid_simstats_tbl
  )

outsample_plots_lst$multi_plot |> print()
outsample_plots_lst$total_plot |> print()
outsample_plots_lst$quant_plot |> print()
```




# R Environment

```{r show_session_info}
#| echo: TRUE
#| message: TRUE

options(width = 120L)
sessioninfo::session_info()
options(width = 80L)
```
